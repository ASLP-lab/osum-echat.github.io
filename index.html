<!DOCTYPE html>
<!-- saved from url=(0033)https://QicongXie.github.io/end2endvc/ -->
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


  <!-- Begin Jekyll SEO tag v2.7.1 -->
  <title>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via Understanding-Driven Spoken Dialogue</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="title">
  <meta property="og:locale" content="en_US">
  <meta name="twitter:card" content="summary">
  <!-- End Jekyll SEO tag -->

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
  <style>
    /* 导航栏样式 - 使用与主题色一致的颜色 */
    .navbar {
      overflow: hidden;
      background-color: #157878; /* 与页头主题色一致 */
      position: sticky;
      top: 0;
      width: 100%;
      z-index: 100;
    }

    .navbar a {
      float: left;
      display: block;
      color: white; /* 白色文字与主题色形成对比 */
      text-align: center;
      padding: 14px 16px;
      text-decoration: none;
      font-size: 17px;
    }

    .navbar a:hover {
      background-color: #0f5e5e; /* 深色变体作为悬停效果 */
      color: white;
    }

    .method {
      display: inline-block;
      font-weight: bold;
    }

    .explanation {
      display: inline-block;
    }

    .video-title {
      text-align: center;
      font-weight: bold;
      margin-top: 10px;
    }
  </style>
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">

  <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
  </section>

  <!-- 导航栏 -->
  <div class="navbar">
    <a href="#abstract">Abstract</a>
    <a href="#demos">Demos</a>
    <a href="#performance">Performance</a>
  </div>

  <section class="main-content">
    <h1 id="">
      <center>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via Understanding-Driven Spoken Dialogue</center>
    </h1>

    <h3 id="">
      <center><a href="http://www.npu-aslp.org/" target="_blank" style="color: inherit; text-decoration: underline;">
          Audio, Speech and Language Processing Group (ASLP@NPU)
        </a> , School of Computer Science, Northwestern Polytechnical University </center>
    </h3>

    <tr>
      <center><img src='raw/fig/SUM.png' width="55%"></center>
    </tr>

    <br>
    <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
    <p>Empathy is crucial in enabling natural interactions within spoken dialogue systems, allowing machines to recognize and respond appropriately to paralinguistic cues such as age, gender, and emotion. Recent advancements in end-to-end speech language models, which unify speech understanding and generation, provide promising solutions. However, several challenges persist, including an over-reliance on large-scale dialogue datasets, insufficient extraction of paralinguistic cues vital for conveying empathy, and the lack of empathy-specific datasets and evaluation frameworks. To address these issues, we introduce OSUM-EChat, an open-source, end-to-end spoken dialogue system designed to enhance empathetic interactions, particularly in resource-limited settings. Based on <a href="https://github.com/ASLP-lab/OSUM/tree/main/OSUM" target="_blank" rel="noopener noreferrer">OSUM</a>, OSUM-EChat introduces two key innovations: (1) a three-stage understanding-driven spoken dialogue training strategy that extends the capabilities of a large speech understanding model to spoken dialogue tasks, and (2) a linguistic-paralinguistic dual thinking mechanism that integrates paralinguistic understanding through a chain of thought with dialogue generation, enabling the system to produce more empathetic responses. This approach reduces reliance on large-scale dialogue datasets while maintaining high-quality empathetic interactions. Additionally, we introduce the EChat-200K dataset, a rich corpus of empathetic speech-to-speech dialogues, and the EChat-eval benchmark, a comprehensive framework for evaluating the empathetic capabilities of dialogue systems. Experimental results demonstrate that OSUM-EChat outperforms end-to-end spoken dialogue models regarding empathetic responsiveness, validating its effectiveness.</p>
    <br><br>
    <table border=0 frame=void rules=none>
      <tr>
        <center><img src='raw/fig/system.png' width="80%"></center>
        <center><span><b>Figure 1: The overview of the architecture and tasks of OSUM-EChat.</b></span> </center>
      </tr>
    </table>
    <br><br>

    <h2 id="demos">2. Demos <a name="Comparison"></a></h2>
    <div class="demo">
      <p>Check out the demo videos showcasing OSUM-EChat's capabilities.</p>
      <div class="video-container" style="display: flex; flex-direction: column; align-items: center; gap: 50px;">
        <div style="display: flex;  align-items: center; height: auto; flex-direction: column;">
          <div class="video-title" style="margin-bottom: 20px;">Self Introduction</div>
          <video controls width="960" height="540" style="border: 3px solid #000;">
            <source src="raw/samples/self-intro.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>

        <div style="display: flex;  align-items: center; height: auto; flex-direction: column;">
          <div class="video-title" style="margin-bottom: 20px;">Knowledge Q&A </div>
          <video controls width="960" height="540" style="border: 3px solid #000;">
            <source src="raw/samples/knowledge.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>

        <div style="display: flex;  align-items: center; height: auto; flex-direction: column;">
          <div class="video-title" style="margin-bottom: 20px;">Empathetic Dialogue - Emotion</div>
          <video controls width="960" height="540" style="border: 3px solid #000;">
            <source src="raw/samples/emotion.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>

        <div style="display: flex;  align-items: center; height: auto; flex-direction: column;">
          <div class="video-title">Empathetic Dialogue - Age and Gender</div>
          <br><br>
          <video controls width="960" height="540" style="border: 3px solid #000;">
            <source src="raw/samples/age_gender.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>

        <div style="display: flex;  align-items: center; height: auto; flex-direction: column;">
          <div class="video-title" style="margin-bottom: 20px;">Empathetic Dialogue - Sound Event</div>
          <video controls width="960" height="540" style="border: 3px solid #000;">
            <source src="raw/samples/caption.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>

        <div style="display: flex;  align-items: center; height: auto; flex-direction: column;">
          <div class="video-title" style="margin-bottom: 20px;">Voice Cloning</div>
          <video controls width="960" height="540" style="border: 3px solid #000;">
            <source src="raw/samples/voice_clone.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>

        <div style="display: flex;  align-items: center; height: auto; flex-direction: column;">
          <div class="video-title" style="margin-bottom: 20px;">Real-time dialogue</div>
          <video controls width="960" height="540" style="border: 3px solid #000;">
            <source src="raw/samples/freeze_full.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div>

    <div class="performance">
  <h2 id="performance">3. Performance</h2>
  <p>Details about the performance benchmarks and capabilities of OSUM-EChat.</p>

  <h3>Empathetic Voice Dialogue</h3>
  <p>In the evaluation using the EChat-eval benchmark, OSUM-EChat demonstrated excellent performance in empathetic dialogue tasks. It achieved high GPT-4o automatic scores across various empathetic dialogue scenarios, particularly excelling in multi-label scenarios. Additionally, it showed strong capability in processing diverse acoustic events in input speech. Detailed results are presented in Table 1.</p>

  <table>
    <tr>
      <td style="text-align: center;">
        <img src='raw/fig/table1.png' width="65%">
        <p><b>Table 1: Automatic evaluation results on EChat-eval benchmark. Here, 'U-Driven' refers to the understanding-driven spoken dialogue training strategy, and 'Dual Think' refers to the linguistic-paralinguistic dual think mechanism.</b></p>
      </td>
    </tr>
  </table>

  <p>Human evaluation results from EChat-eval further indicate that OSUM-EChat's overall performance surpasses Qwen2.5-Omni. It demonstrated excellent performance in empathetic dialogue test cases within the emotional dimension, though it still lags behind commercial systems. Notably, in empathetic dialogue tasks involving other paralinguistic dimensions (such as age, gender, sound events, etc.), commercial systems currently cannot effectively capture relevant cues. Detailed data is shown in Table 2.</p>

  <p>Ablation experiment results verified that applying the speech understanding model (OSUM) to spoken dialogue tasks, combined with the "linguistic-paralinguistic dual thinking mechanism," can significantly enhance the model's empathetic dialogue capabilities. Specific verification data is presented in Table 1.</p>

  <table>
    <tr>
      <td style="text-align: center;">
        <img src='raw/fig/table3.png' width="65%">
        <p><b>Table 2: Human evaluation results of representative models on the EChat-eval benchmark. † ByteDance's commercial system with response from a single fixed speaker.</b></p>
      </td>
    </tr>
  </table>

  <h3>Basic Voice Capabilities</h3>
  <p>OSUM-EChat demonstrated excellent and stable performance in evaluations of three core voice capabilities: language intelligence, speech understanding, and speech synthesis. Detailed analyses are as follows:</p>

  <h4>(1) Language Intelligence</h4>
  <p>Leveraging large-scale text dialogue data and an internally constructed knowledge-based voice question-answering dataset, OSUM-EChat's language intelligence level is comparable to mainstream industry end-to-end voice dialogue models. Specific evaluation results for spoken question-answering tasks are shown in Table 3.</p>

  <table>
    <tr>
      <td style="text-align: center;">
        <img src='raw/fig/table2.png' width="100%">
        <p><b>Table 3: Performance on VoiceBench Benchmarks.</b></p>
      </td>
    </tr>
  </table>

  <h4>(2) Speech Understanding</h4>
  <p>This research verified OSUM-EChat's speech understanding capabilities on open-source test sets for five tasks: Automatic Speech Recognition (ASR), sound event recognition, emotion recognition, age recognition, and gender recognition. Results indicate that its performance is roughly equivalent to the speech understanding large model OSUM (with Qwen-7B as its LLM base) and approaches the level of the industrial-grade speech understanding model Qwen2-Audio.</p>

  <table>
    <tr>
      <td style="text-align: center;">
        <img src='raw/fig/table4.png' width="80%">
        <p><b>Table 4: Performance of speech understanding tasks</b></p>
      </td>
    </tr>
  </table>

     <h4>(3) Speech Synthesis</h4>
<p>In this study, the Text-to-Speech (TTS) capability of OSUM-EChat was evaluated on the SEED test set. The results show that its TTS performance is superior to that of the CosyVoice model; however, there is still a gap when compared with industrial-grade voice dialogue models and professional TTS models. Detailed metrics (Word Error Rate, Character Error Rate) are presented in Table 5.</p>
<table>
<tr>
<td style="text-align: center;">
<img src='raw/fig/table5.png' width="80%">
<p><b>Table 5: Performance comparison between OSUM-EChat and recent spoken dialogue models on the SEED test set (Unit: %, ↓ indicates better performance of the metric)</b></p>
</td>
</tr>
</table>
</div>





    <br><br><br>

    <tr>
      <center><img src='raw/fig/ASLP.jpg' width="35%"></center>
    </tr>
  </section>
</body>

</html>